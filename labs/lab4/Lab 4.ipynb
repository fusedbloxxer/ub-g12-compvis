{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f50b9b0",
   "metadata": {},
   "source": [
    "# Image Stitching \n",
    "<img src=\"images\\stitched.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9596e9",
   "metadata": {},
   "source": [
    "In this lab, we are going to: \n",
    "- Detect feature points in images.\n",
    "- Calculate descriptors for every keypoint in images using SIFT.\n",
    "- Compute feature-space distances between every pair of feature points from image source and destination.\n",
    "- Select good matches (using Lowe's ratio test) based on the distance matrix above.\n",
    "- Get the homography matrix using the RANSAC algorithm.\n",
    "- Generate the panorama from the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95065ad",
   "metadata": {},
   "source": [
    "## Let's understand the stitching algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a1299",
   "metadata": {},
   "source": [
    "In the beginning, we have two images: the **destination image** (left side) and the **source image** (right side). \n",
    "\n",
    "We want to \"transform\" the **source image** with respect to the **destination image**, such that the two images will be well aligned. We accomplish this by applying a perspective transformation to the **source image**. We obtain the perspective transformation based on the matches between the two images.\n",
    "<table width=\"950px\">\n",
    "<tr>\n",
    "<th><center>Destination</center></th>\n",
    "<th><center>Source</center></th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images\\dest.jfif\" width=350 /></td>\n",
    "<td> <img src=\"images\\source.jfif\" width=350 /></td> \n",
    "</tr>\n",
    "</table>   \n",
    "\n",
    "Now, we made the **source image** to have the same persective as the **destination image**.\n",
    "<table width=\"950px\">\n",
    "<tr>\n",
    "<th><center>Destination</center></th>\n",
    "<th><center>Result Source</center></th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images\\dest.jfif\" width=350 /></td>\n",
    "<td> <img src=\"images\\result.png\" width=510 /></td> \n",
    "</tr>\n",
    "</table>\n",
    "    \n",
    "And in the end, we copy the destination image in the **result source** image.\n",
    "<img src=\"images\\stitched_2.png\" width=510 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f842b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import cv2 as cv\n",
    "import typing as ty\n",
    "import pdb\n",
    "import numpy as np\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4eb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_, window_name='image', timeout=0):\n",
    "    \"\"\"\n",
    "    Show image.\n",
    "    :param image_\n",
    "    :param window_name\n",
    "    :param timeout\n",
    "    \"\"\"\n",
    "    cv.imshow(window_name, np.uint8(image_))\n",
    "    cv.waitKey(timeout)\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f68f8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints_and_features(image, show_details=False) -> tuple:\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    1. Convert the image to grayscale.\n",
    "    2. Create the SIFT object. (https://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html)\n",
    "    3. Find keypoints from the grayscale image.\n",
    "    4. Compute the features based on the grayscale image and the keypoints.\n",
    "\n",
    "    :param image.\n",
    "    :return the keypoints: [cv.Keypoint] and the features: np.ndarray for each keypoint.\n",
    "    \"\"\"\n",
    "\n",
    "    def show_keypoints(image_, keypoints_):\n",
    "        \"\"\"\n",
    "        Show the keypoints found in the image.\n",
    "        \"\"\"\n",
    "        image_output = image_.copy()\n",
    "        image_output = cv.drawKeypoints(\n",
    "            image, keypoints_, image_output, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        show_image(image_output, 'keypoints')\n",
    "\n",
    "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    sift = cv.xfeatures2d.SIFT_create()\n",
    "    keypoints, features = sift.detectAndCompute(gray_image, None)\n",
    "\n",
    "    if show_details:\n",
    "        show_keypoints(image, keypoints)\n",
    "\n",
    "    return keypoints, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7134eb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1.905] global shadow_sift.hpp:13 SIFT_create DEPRECATED: cv.xfeatures2d.SIFT_create() is deprecated due SIFT tranfer to the main repository. https://github.com/opencv/opencv/issues/16736\n"
     ]
    }
   ],
   "source": [
    "img_test = cv.imread('data/stitches/2/2.jfif')\n",
    "k1, f1 = get_keypoints_and_features(img_test, show_details=False)\n",
    "img_test2 = cv.imread('data/stitches/2/3.jfif')\n",
    "k2, f2 = get_keypoints_and_features(img_test2, show_details=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf33028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(features_source, features_dest) -> ty.List[ty.List[cv.DMatch]]:\n",
    "    \"\"\"\n",
    "    Match features from the source image with the features from the destination image.\n",
    "    :return: [[DMatch]] - The rusult of the matching. For each set of features from the source image,\n",
    "    it returns the first 'K' matchings from the destination images.\n",
    "    \"\"\"\n",
    " \n",
    "    feature_matcher = cv.DescriptorMatcher_create(\"FlannBased\")\n",
    "    matches = feature_matcher.knnMatch(features_source, features_dest, k=2)   \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379dd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_features = match_features(f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c32ba21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286.7542419433594, 362.1187744140625, < cv2.DMatch 0x7fa9abce3e90>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_features[0][0].distance, matched_features[0][1].distance, matched_features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05873cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as ty\n",
    "\n",
    "\n",
    "def generate_homography(all_matches:  ty.List[cv.DMatch], keypoints_source: ty.List[cv.KeyPoint], keypoints_dest: ty.List[cv.KeyPoint],\n",
    "                        ratio: float = 0.75, ransac_rep: int = 4.0):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    1. Find the matchings that pass the Lowe's ratio test (ratio parameter).\n",
    "    2. Get the coordinates of the keypoints from the source image.\n",
    "    3. Get the coordinates of the keypoints from the destination image.\n",
    "    4. Obtain the Homagraphy. (https://docs.opencv.org/master/d1/de0/tutorial_py_feature_homography.html)\n",
    "    :param all_matches [DMatch]\n",
    "    :param keypoints_source [cv.Point]\n",
    "    :param ratio - Lowe's ratio test (the ratio 1st neighbour distance / 2nd neighbour distance)\n",
    "    :param keypoints_source: nd.array [Nx2] (x, y coordinates)\n",
    "    :param keypoints_dest: nd.array [Nx2] (x, y coordinates)\n",
    "    :param ransac_rep: float. The threshold in the RANSAC algorithm.\n",
    "    :return: The homography matrix.\n",
    "\n",
    "    class DMatch:\n",
    "        distance - Distance between descriptors. The lower, the better it is.\n",
    "        imgIdx - Index of the train image\n",
    "        queryIdx - Index of the descriptor in query descriptors\n",
    "        trainIdx - Index of the descriptor in train descriptors\n",
    "\n",
    "    class KeyPoint:\n",
    "        pt - The x, y coordinates of a point.\n",
    "\n",
    "    \"\"\"\n",
    "    if not all_matches:\n",
    "        return None\n",
    "    matches: ty.List[cv.DMatch] = []\n",
    "    for match in all_matches:\n",
    "        if len(match) == 2 and match[0].distance / match[1].distance < ratio:\n",
    "            matches.append(match[0])\n",
    "    points_source = np.float32([keypoints_source[m.queryIdx].pt for m in matches])\n",
    "    points_destin = np.float32([keypoints_dest[m.trainIdx].pt for m in matches])\n",
    "    if len(points_source) > 4:\n",
    "        H, _ = cv.findHomography(\n",
    "            points_source, points_destin, cv.RANSAC, ransac_rep)\n",
    "        return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea5d0fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.11373056e+00, -1.41136727e-02, -1.51503046e+02],\n",
       "       [ 6.04398631e-02,  1.06568490e+00,  8.29887469e+00],\n",
       "       [ 3.10572413e-04, -5.42831673e-05,  1.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = generate_homography(matched_features, k1, k2)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f044e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images(image_source, image_dest, show_details=False):\n",
    "    \"\"\" \n",
    "    :param image_source (image from the right part).\n",
    "    :param image_dest (image from the left part).\n",
    "    :param show_details\n",
    "    :return - the stitched image.\n",
    "    TODO:\n",
    "    1. Get the keypoints and the features from the source image.\n",
    "    2. Get the keypoints and the features from the destination image.\n",
    "    3. Match the features.\n",
    "    4. Find the homography matrix.\n",
    "    5. Apply the homography matrix on the source image.\n",
    "    (https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html#affine-transformation)\n",
    "    6. Copy the destination image in the resulting image from the previous point.\n",
    "    \"\"\"\n",
    "    keypoints_source, features_source = get_keypoints_and_features(image_source, show_details=show_details)\n",
    "    keypoints_dest, features_dest = get_keypoints_and_features(image_dest, show_details=show_details)\n",
    "\n",
    "    def show_matches(all_matches_, n=10):\n",
    "        matches = sorted(all_matches_, key = lambda x:x[0].distance)\n",
    "        matches = matches[:n] \n",
    "        image_output = cv.drawMatchesKnn(image_source, keypoints_source,\n",
    "                                         image_dest, keypoints_dest,\n",
    "                                         matches, None, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        show_image(image_output, 'matches')\n",
    "\n",
    "    # TODO:\n",
    "    all_matches = match_features(features_source, features_dest)\n",
    "    if show_details:\n",
    "        show_matches(all_matches_=all_matches)\n",
    "    H = generate_homography(all_matches, keypoints_source, keypoints_dest)\n",
    "\n",
    "    if show_details:\n",
    "        show_matches(copy.copy(all_matches))\n",
    "    \n",
    "    result = cv.warpPerspective(image_source, H, (image_dest.shape[1] + image_source.shape[1], image_dest.shape[0]))\n",
    "    result[:image_dest.shape[0], :image_dest.shape[1]] = image_dest\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa7da1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = cv.imread('data/stitches/13/1.jpg')\n",
    "img_test2 = cv.imread('data/stitches/13/2.jpg')\n",
    "stitched_image = stitch_images(img_test2, img_test, show_details=False)\n",
    "show_image(stitched_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb0b934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image(image_, procent=0.1):\n",
    "    pad_h = int(image_.shape[0] * procent)\n",
    "    pad_w = int(image_.shape[1] * procent)\n",
    "    big_image = np.zeros((image_.shape[0] + 2 * pad_h, image_.shape[1] + pad_w, 3), np.uint8)\n",
    "    big_image[pad_h: pad_h + image_.shape[0], pad_w: pad_w + image_.shape[1]] = image_.copy()\n",
    "    return big_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be8d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images_from_folder(folder_path, show_details=False):\n",
    "    \"\"\"\n",
    "    Stitch the images from the last image to the first.\n",
    "    TODO:\n",
    "    1. Read the images from the folder, sort them (ascending order), \n",
    "    then reverse the list (because we are going to stitch them from the last image to the first).\n",
    "    2. Read the first image (the source image).\n",
    "    3. While you have unread images, read the next image (destination image), \n",
    "    stitch it with the source image then save the resulting image in the source image (in the next step it will be the source image).\n",
    "    \"\"\"\n",
    "    image_names = os.listdir(folder_path)\n",
    "    image_names.sort()\n",
    "    image_names = image_names[::-1]\n",
    "    \n",
    "    assert len(image_names) >= 2\n",
    "    result = ... \n",
    "        \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82a477ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stitched \u001b[39m=\u001b[39m stitch_images_from_folder(\u001b[39m'\u001b[39m\u001b[39mdata/stitches/2\u001b[39m\u001b[39m'\u001b[39m, show_details\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m show_image(stitched)\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mshow_image\u001b[0;34m(image_, window_name, timeout)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_image\u001b[39m(image_, window_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    Show image.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    :param image_\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m    :param window_name\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m    :param timeout\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     cv\u001b[39m.\u001b[39mimshow(window_name, np\u001b[39m.\u001b[39;49muint8(image_))\n\u001b[1;32m      9\u001b[0m     cv\u001b[39m.\u001b[39mwaitKey(timeout)\n\u001b[1;32m     10\u001b[0m     cv\u001b[39m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "stitched = stitch_images_from_folder('data/stitches/2', show_details=True)\n",
    "show_image(stitched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch all available images\n",
    "base_folder = 'data/stitches'\n",
    "folder_names = os.listdir(base_folder)\n",
    "for folder_name in folder_names:\n",
    "    stitched = stitch_images_from_folder(folder_path=os.path.join(base_folder, folder_name))\n",
    "    stitched = cv.resize(stitched, None, fx=0.5, fy=0.5)  # use this only if you have a small screen\n",
    "    show_image(stitched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b67f8",
   "metadata": {},
   "source": [
    "## Stitch image Now and Then \n",
    "In this scenario, we are not going to merge (stitched) the images, but to put the content of the source image where it belongs in the destination image.\n",
    "\n",
    "In the left side, we have the \"*now*\" image (**destination**) and in the right part we have the \"*then*\" image (**source**).\n",
    "\n",
    "\n",
    "<table width=\"950px\">\n",
    "<tr>\n",
    "<th><center>Now</center></th>\n",
    "<th><center>Then</center></th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images\\2now.png\" width=350 allign=\"left\"/></td>\n",
    "<td> <img src=\"images\\2then.png\" width=250 /></td> \n",
    "</tr>\n",
    "</table>   \n",
    "\n",
    "Now, we made the ***then* image** to have the same perspective as the ***now* image**.\n",
    "<table width=\"950px\">\n",
    "<tr>\n",
    "<th><center>Now</center></th>\n",
    "<th><center>Result then</center></th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images\\2now.png\" width=350 /></td>\n",
    "<td> <img src=\"images\\result_source.png\" width=350 /></td> \n",
    "</tr>\n",
    "</table>\n",
    "    \n",
    "And in the end, we copy the destination image in the **result then** image, but without replacing the pixels that are already occupied by the *then* image.\n",
    "<img src=\"images\\result_now_then.png\" width=350 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45d7e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images_inside(image_source, image_dest, show_details=False):\n",
    "    \"\"\" \n",
    "    :param image_source (image from the right part).\n",
    "    :param image_dest (image from the lest part).\n",
    "    :param show_details\n",
    "    :return: the stitched image.\n",
    "    TODO:\n",
    "    1. Get the keypoints and the features from the source image.\n",
    "    2. Get the keypoints and the features from the destination image.\n",
    "    3. Match the features.\n",
    "    4. Find the homography matrix.\n",
    "    5. Apply the homography matrix on the source image.\n",
    "    6. Copy the destination image in the resulting image from the previous point, but keep the resulting pixels in place!\n",
    "    \"\"\"\n",
    "    keypoints_source, features_source = get_keypoints_and_features(image_source, show_details=show_details)\n",
    "    keypoints_dest, features_dest = get_keypoints_and_features(image_dest, show_details=show_details)\n",
    " \n",
    "    def show_matches(all_matches_, n=10):\n",
    "        matches = sorted(all_matches_, key = lambda x:x[0].distance)\n",
    "        matches = matches[:n] \n",
    "        image_output = cv.drawMatchesKnn(image_source, keypoints_source, \n",
    "                                         image_dest, keypoints_dest,  \n",
    "                                         matches, None, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        show_image(image_output, 'matches')\n",
    " \n",
    "\n",
    "    # TODO:\n",
    "    all_matches = match_features(features_source, features_dest)\n",
    "    if show_details:\n",
    "        show_matches(all_matches_=all_matches)\n",
    "    H = generate_homography(all_matches, keypoints_source, keypoints_dest)\n",
    "\n",
    "    if show_details:\n",
    "        show_matches(copy.copy(all_matches))\n",
    "    \n",
    "    result = cv.warpPerspective(image_source, H, (image_dest.shape[1], image_dest.shape[0]))\n",
    "    mask = result[:, :] == np.array([0])\n",
    "    result = result * (1 - mask) + image_dest * mask\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8073538",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_now = cv.imread('data/nowthen/2now.png')\n",
    "image_then = cv.imread('data/nowthen/2then.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33a951a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched = stitch_images_inside(image_source=image_then, image_dest=image_now, show_details=True)\n",
    "show_image(stitched) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch all available now/then images\n",
    "base_folder = 'data/nowthen'\n",
    "image_names = os.listdir(base_folder)\n",
    "num_images = len(image_names) // 2\n",
    "for i in range(1, num_images + 1):\n",
    "    image_now = cv.imread(os.path.join(base_folder, f'{i}now.png'))\n",
    "    image_then = cv.imread(os.path.join(base_folder, f'{i}then.png'))\n",
    "    stitched = stitch_images_inside(image_source=image_then, image_dest=image_now, show_details=False)\n",
    "    show_image(stitched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11cfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
